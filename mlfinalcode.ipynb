{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML- Random Forest Code \n",
        "\n",
        "## Breast Cancer Diagnosis\n"
      ],
      "metadata": {
        "id": "m2K5QU9IicNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the needed Libraries and packages "
      ],
      "metadata": {
        "id": "Wd3yMXdjikAb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sFQZriwriQ6t"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "from random import seed\n",
        "import random \n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from math import sqrt\n",
        "from sklearn.metrics import confusion_matrix, f1_score \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "from google.colab import drive\n",
        "import os\n",
        "from random import sample"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive "
      ],
      "metadata": {
        "id": "VfUPLTM7in2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmKdH84riZaM",
        "outputId": "930809bf-d045-4165-d5ca-2a777c4dfd65"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set file path and change directory to our specified directory "
      ],
      "metadata": {
        "id": "m9ASYC3xipBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filepath \n",
        "filepath= \"/content/drive/MyDrive/ML-project/data_final.csv\"\n",
        "\n",
        "#change directory \n",
        "os.chdir(\"/content/drive/MyDrive/ML-project/\")"
      ],
      "metadata": {
        "id": "zkKuNCd4ibCs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the Dataset"
      ],
      "metadata": {
        "id": "j87bnd1MjAnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load our csv data file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset"
      ],
      "metadata": {
        "id": "A4TgOorbi4yh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the string column entries to floats\n",
        "def str_column_to_float(dataset, column):\n",
        "\t# loop over dataset\n",
        "\tfor row in dataset:\n",
        "\t\t# convert the string to float \n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "# function to convert the string column entries to integers\n",
        "def str_column_to_int(dataset, column):\n",
        "    # create set of all vals in the col\n",
        "    values = set(row[column] for row in dataset)\n",
        "    # create dictionary mapping each val to integer index\n",
        "    value_map = {value: i for i, value in enumerate(values)}\n",
        "    # iterate over each row in the dataset and replace the string value with its corresponding integer value\n",
        "    for row in dataset:\n",
        "        row[column] = value_map[row[column]]"
      ],
      "metadata": {
        "id": "RxOZQLUmjCK-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for Data Separation and Cross Validation"
      ],
      "metadata": {
        "id": "row6Luu8jO_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to split the dataset into k specified folds\n",
        "def cross_validation_split(dataset, n_folds): \n",
        "    # copy the dataset \n",
        "    dataset_copy = dataset.copy()\n",
        "    # get size of each fold\n",
        "    fold_size = len(dataset) // n_folds\n",
        "    # empty list to store the folds\n",
        "    folds = []\n",
        "    # loop through the k folds \n",
        "    for i in range(n_folds):\n",
        "        # sample without replacement to get the datapoints in this fold \n",
        "        fold = sample(dataset_copy, fold_size)\n",
        "        # remove the samples from the dataset so they don't appear in other folds\n",
        "        dataset_copy = [data for data in dataset_copy if data not in fold]\n",
        "        # add the samples to the list of folds\n",
        "        folds.append(fold)\n",
        "    # return the list of folds\n",
        "    return folds"
      ],
      "metadata": {
        "id": "azs44-N8jSUX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to split our dataset based on a feature and its corresaponding value\n",
        "def test_split(index, value, dataset):\n",
        "    # split dataset into left and right nodes \n",
        "\t\t# based on the specified feature and value\n",
        "\t\tleft = [row for row in dataset if row[index] < value]\n",
        "\t\tright = [row for row in dataset if row[index] >= value]\n",
        "\t\t# return the left and right splits \n",
        "\t\treturn left, right"
      ],
      "metadata": {
        "id": "jfcJ5htAjpOA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create a random subsample from the dataset with replacement\n",
        "def subsample(dataset, ratio):\n",
        "    # get size of subsample\n",
        "    n_sample = round(len(dataset) * ratio)\n",
        "    # empty list to store subsample\n",
        "    sample = []\n",
        "    # loop through the dataset \n",
        "    for i in range(n_sample):\n",
        "        # select an index randomly \n",
        "        index = random.randrange(len(dataset))\n",
        "        # add the row to the subsample \n",
        "        sample.append(dataset[index])\n",
        "    return sample"
      ],
      "metadata": {
        "id": "xgFu7AKukNZc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for Random Forest Construction "
      ],
      "metadata": {
        "id": "wtFtvbIbjwY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to build the decision tree\n",
        "def construct_tree(train, max_depth, min_size, n_features):\n",
        "\t# make the root \n",
        "\troot = create_split_point(train, n_features)\n",
        "\t# split the re4st of the dataset recursively to make the whole tree \n",
        "\tactual_split(root, max_depth, min_size, n_features, 1)\n",
        "\treturn root"
      ],
      "metadata": {
        "id": "RU-e_FlukFJW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to select the best split point\n",
        "def create_split_point(dataset, n_features):\n",
        "\t# get list of unique class vals\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\t# variables to store the best split point found so far\n",
        "\tbest_index, best_value, best_score, best_groups = 999, 999, 999, None \n",
        "\tfeatures = []\n",
        "\t# list of features to consider for split\n",
        "\twhile len(features) < n_features:\n",
        "\t\tindex = random.randrange(len(dataset[0])-1)\n",
        "\t\tif index not in features:\n",
        "\t\t\tfeatures.append(index)\n",
        "\t# loop through each feature and each row to find best split point\n",
        "\tfor index in features:\n",
        "\t\tfor row in dataset:\n",
        "\t\t\t# split dataset based on the current feature and row \n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\t# get the gini index of the split\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\t# update the best split point if the current split has a lower gini index\n",
        "\t\t\tif gini < best_score:\n",
        "\t\t\t\tbest_index, best_value, best_score, best_groups = index, row[index], gini, groups\n",
        "\t# return a dictionary with the best split point found\n",
        "\treturn {'index': best_index, 'value': best_value, 'groups': best_groups}"
      ],
      "metadata": {
        "id": "F4_N0HHlj5ck"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that creates child splits for a node or makes the node terminal\n",
        "def actual_split(node, max_depth, min_size, n_features, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# if no split, then the node is a terminal node \n",
        "\tif not left or not right:\n",
        "\t\t# make node left and righ terminal \n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\t# if more than max depth, stop expamding th etree and make the nodes terminal \n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# if the length is less than the max depth, continue divoding the tree \n",
        "\t# left child\n",
        "\t# if size is less than min size needed for a node, make it terminal \n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\t# if size is large enough for datapoints under this node, choose an attribute to split on and split \n",
        "\t\tnode['left'] = create_split_point(left, n_features)\n",
        "\t\tactual_split(node['left'], max_depth, min_size, n_features, depth+1)\n",
        "\t# right child\n",
        "\t# if size is less than min size needed for a node, make it terninal \n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\t# if size is large enough for datapoints under this node, choose an attribute to split on and split \n",
        "\t\tnode['right'] = create_split_point(right, n_features)\n",
        "\t\tactual_split(node['right'], max_depth, min_size, n_features, depth+1)"
      ],
      "metadata": {
        "id": "BCKDVRvGj_mr"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create a terminal node value\n",
        "def to_terminal(subset):\n",
        "\t# set the node label as the majoity label of the corresponding rows in the subset\n",
        "\toutcomes = [row[-1] for row in subset]\n",
        "\treturn max(set(outcomes), key=outcomes.count)"
      ],
      "metadata": {
        "id": "tuE58o9tj8os"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate the gini index for the dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# get all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum the weighted gini index for each group\n",
        "\tgini = 0.0\n",
        "\t# gor each group \n",
        "\tfor g in groups:\n",
        "\t\t# get size of group\n",
        "\t\tsize = float(len(g))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# get the score of the group based on the score for each class\n",
        "\t\tfor val in classes:\n",
        "\t\t\tp = [row[-1] for row in g].count(val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\t# return the gini index \n",
        "\treturn gini"
      ],
      "metadata": {
        "id": "N_ROuVgAjy8D"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codes For Plotting, Visualization, and Calculating Performance  "
      ],
      "metadata": {
        "id": "W4JrV_C8ivQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "    # check if feature val of given row is less than the node split value \n",
        "\t\t# here, feature value less than node -> go to left \n",
        "    if row[node['index']] < node['value']:\n",
        "        # if left child of the node is a dictionary, we already have a subtree at this node \n",
        "\t\t\t\t# recursively call the predict function with the left child as the new node\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        # if not, no left subtree\n",
        "\t\t\t\t# return the predicted class label = value of the left child\n",
        "        else:\n",
        "            return node['left']\n",
        "    # here, feature value greater than or equal to the split node -> go right \n",
        "    else:\n",
        "        # if right child of the node is a dictionary, we already have a subtree at this node \n",
        "\t\t\t\t# recursively call the predict function with the left child as the new node\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        # if not, no right subtree\n",
        "\t\t\t\t# return the predicted class label = value of the right child\n",
        "        else:\n",
        "            return node['right']"
      ],
      "metadata": {
        "id": "JfnOGxBKkIz5"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to make a prediction with a list of bagged trees\n",
        "# def bagging_predict(trees, row):\n",
        "# \t# make predictions for each tree in trees \n",
        "# \tpredictions = [predict(tree, row) for tree in trees]\n",
        "# \t# return the majority prediction from all trees \n",
        "# \treturn max(set(predictions), key=predictions.count)\n",
        "\n",
        "def bagging_predict(trees, row):\n",
        "    # list to store the different tree predictions \n",
        "    predictions = []\n",
        "    # make predictions for each tree in trees\n",
        "    for tree in trees:\n",
        "        prediction = predict(tree, row)\n",
        "        predictions.append(prediction)\n",
        "    # count the occurrence of each prediction (0 or 1)\n",
        "    counts = [predictions.count(p) for p in set(predictions)]\n",
        "    # get the index of the prediction with the max count\n",
        "    max_index = counts.index(max(counts))\n",
        "    # return the prediction with the max count\n",
        "    return list(set(predictions))[max_index]"
      ],
      "metadata": {
        "id": "fhUx6OIkkQwO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate the accuracy \n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0"
      ],
      "metadata": {
        "id": "i6Nt54c0jd0_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to evaluate our algorithm using cross validation \n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores_test = list()\n",
        "\tscores_train = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredictions_test, predictions_train = algorithm(train_set, test_set, *args)\n",
        "\t\n",
        "\t\tactual_test = [row[-1] for row in fold]\n",
        "\t\taccuracy_test = accuracy_metric(actual_test, predictions_test)\n",
        "\t\tf1_score_test = f1_score(actual_test, predictions_test)\n",
        "\t\t# plot_confusion_matrix(actual_test, predictions_test,[0.,1.])\n",
        "\t\n",
        "\t\tactual_train = [row[-1] for row in train_set]\n",
        "\t\taccuracy_train = accuracy_metric(actual_train, predictions_train)\n",
        "\t\tf1_score_train = f1_score(actual_train, predictions_train)\n",
        "\t\t\n",
        "\t\tscores_test.append(accuracy_test)\n",
        "\t\tscores_train.append(accuracy_train)\n",
        "\t\n",
        "\n",
        "\treturn scores_test, scores_train,f1_score_test,f1_score_train"
      ],
      "metadata": {
        "id": "GIAQrjEFjkH6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to plot the confusion matrix \n",
        "def plot_confusion_matrix(actual_classes : np.array, predicted_classes : np.array, sorted_labels : list):\n",
        "\t\tmatrix = confusion_matrix(actual_classes, predicted_classes, labels=sorted_labels)\n",
        "\t\tmatrix= matrix.astype('float') / matrix.sum(axis=1)[:,np.newaxis]\n",
        "\t\tplt.figure(figsize=(12.8,6))\n",
        "\t\tsns.heatmap(matrix, annot=True, xticklabels=sorted_labels, yticklabels=sorted_labels, cmap=\"RdPu\", fmt=\"g\")\n",
        "\t\tplt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title('Confusion Matrix')\n",
        "\t\t# if title == None:\n",
        "\t\t# \tplt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title('Confusion Matrix')\n",
        "\t\t# else:\n",
        "\t\t# \tplt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title(title)\n",
        "\t\tplt.show()"
      ],
      "metadata": {
        "id": "Xt8aEoYlisWe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running Random Forest"
      ],
      "metadata": {
        "id": "LQnncA-5kVkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the function for running the random forest algorithm\n",
        "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
        "\ttrees = list()\n",
        "\tfor i in range(n_trees):\n",
        "\t\tsample = subsample(train, sample_size)\n",
        "\t\ttree = construct_tree(sample, max_depth, min_size, n_features)\n",
        "\t\ttrees.append(tree)\n",
        "\tpredictions_train = [bagging_predict(trees, row) for row in train]\n",
        "\tpredictions_test = [bagging_predict(trees, row) for row in test]\n",
        "\n",
        "\treturn(predictions_test,predictions_train)"
      ],
      "metadata": {
        "id": "w3sWQmtkkVBV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our test of Random Forest "
      ],
      "metadata": {
        "id": "w6t3i04ikecw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the random forest algorithm\n",
        "seed(2)\n",
        "\n",
        "# load and prepare data\n",
        "filename = '/content/drive/MyDrive/ML-project/data_final.csv'\n",
        "dataset = load_csv(filename)\n",
        "\n",
        "# convert string attributes to integers\n",
        "for i in range(0, len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "\n",
        "# evaluate algorithm\n",
        "# set the number of folds for cross validation, the tree max depth and min size, the number of features at each level, and initialize the array of accuracies \n",
        "n_folds = 5\n",
        "max_depth = 10\n",
        "min_size = 1\n",
        "sample_size = 1.0\n",
        "n_features = int(sqrt(len(dataset[0])-1))\n",
        "num_trees_lower = 5\n",
        "num_trees_upper = 10 + 1\n",
        "all_mean_accuracies = [0]*(num_trees_upper-num_trees_lower)\n",
        "i = 0\n",
        "\n",
        "# loop over the range of forests with n_trees in each forest\n",
        "for n_trees in range(num_trees_lower,num_trees_upper):\n",
        "  # for each forest size within the rage, get test,train, f1 scores \n",
        "\tscores_test, scores_train,f1_score_test,f1_score_train = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
        "\t# get the mean accuracy \n",
        "\tmean_test_accuracy = sum(scores_test)/float(len(scores_test))\n",
        "\tall_mean_accuracies[i] = mean_test_accuracy\n",
        "\t\n",
        "\tprint('Number of Trees in our Forest: %d' % n_trees)\n",
        "\tprint('Testing Scores: %s' % scores_test)\n",
        "\tprint('Testing Mean Accuracy: %.3f%%' % (mean_test_accuracy))\n",
        "\tprint('F1 Score: %.3f%%' % (f1_score_test*100))\n",
        "\tprint('Training Scores: %s' % scores_train)\n",
        "\tprint('Training Mean Accuracy: %.3f%%' % (sum(scores_train)/float(len(scores_train))))\n",
        "\tprint('______________')\n",
        "\ti += 1"
      ],
      "metadata": {
        "id": "5DDFBnkwkbHp",
        "outputId": "43ec5a3d-9f74-4a67-edce-d3ab3158dda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Trees in our Forest: 5\n",
            "Testing Scores: [93.80530973451327, 93.80530973451327, 94.69026548672566, 94.69026548672566, 93.80530973451327]\n",
            "Testing Mean Accuracy: 94.159%\n",
            "F1 Score: 95.238%\n",
            "Training Scores: [98.45132743362832, 99.33628318584071, 99.11504424778761, 98.67256637168141, 99.77876106194691]\n",
            "Training Mean Accuracy: 99.071%\n",
            "______________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-8952c7704bd6>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_trees\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trees_lower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_trees_upper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m# for each forest size within the rage, get test,train, f1 scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mscores_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# get the mean accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmean_test_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-dbf410e55bef>\u001b[0m in \u001b[0;36mevaluate_algorithm\u001b[0;34m(dataset, algorithm, n_folds, *args)\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0mrow_copy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mpredictions_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mactual_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-990fa80647b9>\u001b[0m in \u001b[0;36mrandom_forest\u001b[0;34m(train, test, max_depth, min_size, sample_size, n_trees, n_features)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpredictions_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbagging_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9cf749f02c5c>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(train, max_depth, min_size, n_features)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# function to build the decision tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-68d64ffe9dd6>\u001b[0m in \u001b[0;36mget_split\u001b[0;34m(dataset, n_features)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0;31m# split dataset based on the current feature and row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                         \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                         \u001b[0;31m# get the gini index of the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                         \u001b[0mgini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgini_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-c9d52af39ea1>\u001b[0m in \u001b[0;36mtest_split\u001b[0;34m(index, value, dataset)\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0;31m# based on the specified feature and value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0;31m# return the left and right splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-c9d52af39ea1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0;31m# based on the specified feature and value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0;31m# return the left and right splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}